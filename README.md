# ğŸš€ GCP Batch Data Pipeline with CLI + Dashboard

This project demonstrates a fully automated batch data pipeline using Google Cloud Platform tools, Python, and Looker Studio.

## ğŸ”§ Technologies Used
- Python 3
- Google Cloud Storage (GCS)
- BigQuery
- Looker Studio
- gcloud CLI
- bq CLI

## ğŸ“Š Pipeline Overview

1. Fetch data from a public API
2. Save data as NDJSON
3. Upload to Google Cloud Storage
4. Load into BigQuery
5. Perform SQL-based transformations
6. Export to GCS as CSV
7. Visualize in Looker Studio

## ğŸ“‚ Files

- `fetchdata457api.py` â€” Python script to fetch and store API data
- `pipeline.bat` â€” Automates the entire ETL flow
- `data/sample_data.json` â€” Sample NDJSON file
- `diagrams/pipeline_diagram.png` â€” Architecture overview
- `requirements.txt` â€” Python libraries

## ğŸ” Run the Pipeline

```bash
# Run from CLI
pipeline.bat
```

## ğŸ“ˆ Dashboard

Use the exported CSV from GCS as a data source in Looker Studio and build your visualizations.